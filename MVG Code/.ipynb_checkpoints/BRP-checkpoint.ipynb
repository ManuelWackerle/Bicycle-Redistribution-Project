{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d453668",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7520/2810901792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeojson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeatureCollection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import json\n",
    "import matplotlib\n",
    "import holidays\n",
    "import h3\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "#import random\n",
    "import matplotlib.pyplot as plt\n",
    "#import geopy.distance\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "#import branca\n",
    "import branca.colormap as cm\n",
    "#from folium import plugins\n",
    "import warnings\n",
    "import statistics\n",
    "import math\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e1127",
   "metadata": {},
   "source": [
    "This is the preprocessing - it is quiet time consuming so you might do it once and load a pickle file afterwards to save time. However, you might want to change parameters later (like cell size) or produce multiple pickle files with different parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15504447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file containing all the rides of 2021 and store it as a DataFrame\n",
    "rides_df = pd.read_csv('MVG_Rad_Fahrten_2021.csv', sep=';', error_bad_lines=False)\n",
    "# we want to get rid of empty spaces that exist in the column headers in the .csv. \n",
    "rides_df.columns = rides_df.columns.str.replace(' ', '')\n",
    "rides_df['RETURN_STATION_NAME'] = rides_df['RETURN_STATION_NAME'].apply(lambda x: re.sub('\\s+', '', x))\n",
    "rides_df['RENTAL_STATION_NAME'] = rides_df['RENTAL_STATION_NAME'].apply(lambda x: re.sub('\\s+', '', x))\n",
    "# convert the format of the dates from string to Pandas Datetime\n",
    "rides_df['STARTTIME'] = pd.to_datetime(rides_df['STARTTIME'])\n",
    "rides_df['ENDTIME'] = pd.to_datetime(rides_df['ENDTIME'])\n",
    "# calculate a duration column from start- and endtime of each trip\n",
    "rides_df['duration'] = rides_df['ENDTIME'] - rides_df['STARTTIME']\n",
    "# convert the duration column to seconds\n",
    "rides_df['duration'] = rides_df['duration'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbaaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if the day is weekend or weekday or holiday. This takes about 3 minutes.\n",
    "# first, we need to get a list of holidays in Bavaria in 2021\n",
    "h = holidays.country_holidays('DE', 'BY', years=[2021])\n",
    "# unfortunately, we have to iterate through all columns - this is terribly slow.\n",
    "for row in rides_df.index:\n",
    "    # check if date is a holiday\n",
    "    if rides_df.loc[row, 'STARTTIME'].date() in h:\n",
    "        rides_df.loc[row, 'is_holiday'] = 1\n",
    "    else:\n",
    "        rides_df.loc[row, 'is_holiday'] = 0\n",
    "    # check if date is weekday or weekend\n",
    "    if rides_df.loc[row, 'STARTTIME'].weekday() > 4:\n",
    "        rides_df.loc[row, 'is_weekend'] = 1\n",
    "    else:\n",
    "        rides_df.loc[row, 'is_weekend'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b61f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we finally add the H3 cell to each ride (start and end). H3 cells are enumerated with a 64-bit integer - this is basically a unique number for each cell. More on the concept and indexing here: https://h3geo.org/docs/core-library/h3Indexing\n",
    "# first, we have to set the resolution (this is basically the size of the cells. 8 is the resolution i have shown you in the examples and I find it suitable to the problem - feel free to play around with it!).\n",
    "# takes around 2.5 minutes\n",
    "resolution = 8 # this is where you could change resolution!\n",
    "# iterating over the DataFrame again - very slow again.\n",
    "for row in(rides_df.index):\n",
    "    rides_df.loc[row, 'h3_start'] = h3.geo_to_h3(lat=rides_df.loc[row, 'STARTLAT'], lng=rides_df.loc[row, 'STARTLON'], resolution=resolution)\n",
    "    rides_df.loc[row, 'h3_end'] = h3.geo_to_h3(lat=rides_df.loc[row, 'ENDLAT'], lng=rides_df.loc[row, 'ENDLON'], resolution=resolution)\n",
    "\n",
    "# we need coordinates of the centres of all cells\n",
    "# retrieve all unique IDs where at least one ride has started or finished\n",
    "hex_ids = list(rides_df[\"h3_start\"].explode().unique()) + list(rides_df[\"h3_end\"].explode().unique())\n",
    "\n",
    "\n",
    "# store the coordinates in a dictionary for later purposes\n",
    "cell_coords = {}\n",
    "#retrieve values and store them\n",
    "for id in hex_ids:\n",
    "    cell_coords[id] = h3.h3_to_geo(id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f70df",
   "metadata": {},
   "source": [
    "now, we have all the information we need to calculate disbalances in our system. What we want (for now) is an average surplus/shortage of bikes per cell per weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03385308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the slowest operation - takes ~ 15 min\n",
    "# first, we filter out the weekends and holidays\n",
    "# wd is short for working days\n",
    "rides_wd = rides_df[(rides_df['is_weekend']==0) & (rides_df['is_holiday']==0)]\n",
    "# we add a date column (without time), will be needed for filtering later\n",
    "rides_wd['STARTDATE'] = rides_wd['STARTTIME'].apply(lambda x: x.date())\n",
    "rides_wd['ENDDATE'] = rides_wd['ENDTIME'].apply(lambda x: x.date())\n",
    "# we dp also need the IDs of the cells as a list to filter \n",
    "hex_ids = list(rides_wd[\"h3_start\"].explode().unique()) + list(rides_wd[\"h3_end\"].explode().unique())\n",
    "# we will create a dictionary where every cell gets a value (the average, median & max disbalance over 24h - so we can decide what's interesting)\n",
    "cell_disbalances_avg = {}\n",
    "cell_disbalances_median = {}\n",
    "cell_disbalances_max = {}\n",
    "cell_disbalances_round = {}\n",
    "cell_disbalances_round_up = {}\n",
    "\n",
    "# now let's start to count...\n",
    "for hex_id in hex_ids:\n",
    "    # first, we filter the DataFrame to only contain the currently relevant cell\n",
    "    tmp_df = rides_wd[(rides_wd['h3_start']==hex_id) | (rides_wd['h3_end']==hex_id)]\n",
    "    # now we need a list of all days so we can filter by that\n",
    "    dates_tmp = set(list(tmp_df[\"STARTDATE\"].explode().unique()) + list(tmp_df[\"ENDDATE\"].explode().unique()))\n",
    "    # we can now create list with the respective values for each date\n",
    "    disbalances_tmp = []\n",
    "    for date in dates_tmp:\n",
    "        disbalances_tmp.append(len(tmp_df[(tmp_df['h3_end']==hex_id) & (tmp_df['ENDDATE']==date)]) - len(tmp_df[(tmp_df['h3_start']==hex_id) & (tmp_df['STARTDATE']==date)]))\n",
    "    # now we have to build the average:\n",
    "    cell_disbalances_avg[hex_id] = statistics.mean(disbalances_tmp)\n",
    "    cell_disbalances_median[hex_id] = statistics.median(disbalances_tmp)\n",
    "    cell_disbalances_max[hex_id] = max(disbalances_tmp)\n",
    "    cell_disbalances_round[hex_id] = round(cell_disbalances_avg[hex_id])\n",
    "    cell_disbalances_round_up[hex_id] = math.ceil(cell_disbalances_avg[hex_id]) if cell_disbalances_avg[hex_id]>0 else math.floor(cell_disbalances_avg[hex_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af54b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a lot of data - to get an idea we can plot it. Feel free to build own (better) vizualizations!\n",
    "x, y = zip(*cell_disbalances_round_up.items())\n",
    "plt.plot(x,y)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(15, 7)\n",
    "plt.grid(visible=True, axis='y')\n",
    "plt.title(\"Disbalance of stations (single weekdays)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_bins = 50\n",
    "bw_adjust = 4\n",
    "sns.histplot(data=cell_disbalances_round_up.items(), kde=True, kde_kws={'bw_adjust':bw_adjust}, bins=nr_bins, label=\"Weekday\")\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(15, 7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48964fc",
   "metadata": {},
   "source": [
    "We can also look at complete working weeks - this oviously leads to higher numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35633522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to consider weeks, we need to add the week number\n",
    "rides_wd['STARTWEEK'] = rides_wd['STARTTIME'].apply(lambda x: x.date().strftime(\"%V\"))\n",
    "rides_wd['ENDWEEK'] = rides_wd['ENDTIME'].apply(lambda x: x.date().strftime(\"%V\"))\n",
    "# we dp also need the IDs of the cells as a list to filter \n",
    "hex_ids = list(rides_wd[\"h3_start\"].explode().unique()) + list(rides_wd[\"h3_end\"].explode().unique())\n",
    "# we will create a dictionary where every cell gets a value (the average, median & max disbalance over 24h)\n",
    "cell_disbalances_avg_week = {}\n",
    "cell_disbalances_median_week = {}\n",
    "cell_disbalances_max_week = {}\n",
    "cell_disbalances_round_week = {}\n",
    "cell_disbalances_round_up_week = {}\n",
    "\n",
    "# now let's start to count...\n",
    "for hex_id in hex_ids:\n",
    "    # first, we filter the DataFrame to only contain the currently relevant cell\n",
    "    tmp_df = rides_wd[(rides_wd['h3_start']==hex_id) | (rides_wd['h3_end']==hex_id)]\n",
    "    # now we need a list of all days so we can filter by that\n",
    "    dates_tmp = set(list(tmp_df[\"STARTDATE\"].explode().unique()) + list(tmp_df[\"ENDDATE\"].explode().unique()))\n",
    "    weeks_tmp = set([x.strftime(\"%V\") for x in dates_tmp])\n",
    "    # we can now create list with the respective values for each date\n",
    "    disbalances_tmp = []\n",
    "    for week in weeks_tmp:\n",
    "        disbalances_tmp.append(len(tmp_df[(tmp_df['h3_end']==hex_id) & (tmp_df['ENDWEEK']==week)]) - len(tmp_df[(tmp_df['h3_start']==hex_id) & (tmp_df['STARTWEEK']==week)]))\n",
    "    # now we have to build the average:\n",
    "    cell_disbalances_avg_week[hex_id] = statistics.mean(disbalances_tmp)\n",
    "    cell_disbalances_median_week[hex_id] = statistics.median(disbalances_tmp)\n",
    "    cell_disbalances_max_week[hex_id] = max(disbalances_tmp)\n",
    "    cell_disbalances_round_week[hex_id] = round(cell_disbalances_avg_week[hex_id])\n",
    "    cell_disbalances_round_up_week[hex_id] = math.ceil(cell_disbalances_avg_week[hex_id]) if cell_disbalances_avg_week[hex_id]>0 else math.floor(cell_disbalances_avg_week[hex_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a lot of data - to get an idea we can plot it. Feel free to build better vizualizations!\n",
    "x, y = zip(*cell_disbalances_round_week.items())\n",
    "plt.plot(x,y)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(20, 10)\n",
    "plt.grid(visible=True, axis='y')\n",
    "plt.title(\"Disbalance of stations (Monday-Friday)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c540a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_bins = 50\n",
    "bw_adjust = 4\n",
    "sns.histplot(data=cell_disbalances_round_week.items(), kde=True, kde_kws={'bw_adjust':bw_adjust}, bins=nr_bins, label=\"Weekday\")\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(20, 10)\n",
    "print(len(hex_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e454a5",
   "metadata": {},
   "source": [
    "Let's have a look on the map! Unfortunately, we therefore need some helper functions. This involves converting our hex cells to linestrings that can be plottet, defining colormaps that match the min/max values and of course creating a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for hexbin plotting\n",
    "def hexagons_dataframe_to_geojson(df_hex, file_output=None, column_name=\"value\"):\n",
    "    \"\"\"\n",
    "    Produce the GeoJSON for a dataframe, constructing the geometry from the \"hex_id\" column\n",
    "    and with a property matching the one in column_name\n",
    "    \"\"\"\n",
    "    list_features = []\n",
    "\n",
    "    for i, row in df_hex.iterrows():\n",
    "        geometry_for_row = {\"type\": \"Polygon\",\n",
    "                            \"coordinates\": [h3.h3_to_geo_boundary(h=row[\"id\"], geo_json=True)]}\n",
    "        feature = Feature(geometry=geometry_for_row, id=str(row[\"id\"]), properties={column_name: int(row[column_name])})\n",
    "        list_features.append(feature)\n",
    "\n",
    "\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "    return geojson_result\n",
    "\n",
    "\n",
    "def get_color(custom_cm, val, vmin, vmax):\n",
    "    return matplotlib.colors.to_hex(custom_cm((val - vmin) / (vmax - vmin)))\n",
    "\n",
    "\n",
    "def choropleth_map(df_aggreg, column_name=\"value\", border_color='black', fill_opacity=0.5, color_map_name=\"Blues\",\n",
    "                   initial_map=None):\n",
    "    \"\"\"\n",
    "    Creates choropleth maps given the aggregated data. initial_map can be an existing map to draw on top of.\n",
    "    \"\"\"\n",
    "    # colormap\n",
    "    min_value = df_aggreg[column_name].min()\n",
    "    max_value = df_aggreg[column_name].max()\n",
    "    mean_value = df_aggreg[column_name].mean()\n",
    "    print(f\"Colour column min value {min_value}, max value {max_value}, mean value {mean_value}\")\n",
    "    print(f\"Hexagon cell count: {df_aggreg['id'].nunique()}\")\n",
    "\n",
    "    # the name of the layer just needs to be unique, put something silly there for now:\n",
    "    name_layer = \"Choropleth \" + str(df_aggreg)\n",
    "\n",
    "    # if no map exists, we build one already zoomed into Munich for our case. \n",
    "    if initial_map is None:\n",
    "        initial_map = folium.Map(location=[48.144975, 11.556093], zoom_start=12, tiles=\"stamentoner\")\n",
    "\n",
    "    # create geojson data from dataframe\n",
    "    geojson_data = hexagons_dataframe_to_geojson(df_hex=df_aggreg, column_name=column_name)\n",
    "\n",
    "    # many more at https://matplotlib.org/stable/tutorials/colors/colormaps.html to choose from!\n",
    "    #custom_cm = matplotlib.cm.get_cmap(color_map_name)\n",
    "    custom_cm_ff = cm.LinearColormap(colors=['red', 'white', 'green'], index=[min_value, 0, max_value], vmin=min_value,vmax=max_value)\n",
    "    custom_cm_ff.caption = 'Delta between returns and rentals'\n",
    "    custom_cm_ff.add_to(initial_map)\n",
    "\n",
    "    # now we put the contents (colored bins) on the base map using folium\n",
    "    folium.GeoJson(\n",
    "        geojson_data,\n",
    "        style_function=lambda feature: {\n",
    "            #'fillColor': get_color(custom_cm, feature['properties'][column_name], vmin=min_value, vmax=max_value),\n",
    "            'fillColor': custom_cm_ff(feature['properties'][column_name]),\n",
    "            'color': border_color,\n",
    "            'weight': 1,\n",
    "            'fillOpacity': fill_opacity, \n",
    "            'popup':feature['properties'][column_name]\n",
    "        },\n",
    "        name=name_layer\n",
    "    ).add_to(initial_map)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return initial_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need a dataframe with a specific shape for the plotting function (and we need to save it for your further calcuations ;). there hast to be a column with the IDs of the cells and values. So we build that DF out of the dictionary\n",
    "\n",
    "def get_x(h3_id):\n",
    "    # returns the x coordinate of a h3 cell\n",
    "    return (cell_coords[h3_id][0])\n",
    "def get_y(h3_id):\n",
    "    # returns the y coordinate of a h3 cell\n",
    "    return (cell_coords[h3_id][1])\n",
    "\n",
    "# this is the DataFrame we will save as csv (ID, delta, coords)- if you change the Dataframe from which we take the items, you can produce another csv.\n",
    "out_df = pd.DataFrame(cell_disbalances_round_up_week.items(), columns=['id', 'delta'])\n",
    "# add the x coordinate\n",
    "out_df['x_coord'] = out_df['id'].apply(lambda x: get_x(x))\n",
    "# add the y coordinate\n",
    "out_df['y_coord'] = out_df['id'].apply(lambda x: get_y(x))\n",
    "\n",
    "# save as csv\n",
    "out_df.to_csv('test_out.csv')\n",
    "#display the top rows of DF\n",
    "out_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's finally take a look at the map! First, we have to build it using the previoously defined helper functions.\n",
    "m_d_wd = choropleth_map(df_aggreg = out_df, column_name='delta', color_map_name = \"viridis\", fill_opacity=0.7)\n",
    "# display it\n",
    "m_d_wd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ee01fd1d8812b36236e8ef1ac919ceb21ec921e0363761052181dca760e2314"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
